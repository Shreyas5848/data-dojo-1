{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312101fa",
   "metadata": {},
   "source": [
    "# üéØ Classification Analysis: customers_messy\n",
    "\n",
    "**Generated:** 2025-12-09 12:50:54  \n",
    "**Type:** Classification Modeling  \n",
    "**Dataset:** customers_messy\n",
    "\n",
    "## üéØ Objective\n",
    "This notebook provides a complete classification modeling workflow including data exploration, preprocessing, model training, and evaluation.\n",
    "\n",
    "## üìã Workflow Steps\n",
    "1. **Data Loading & Exploration**\n",
    "2. **Target Variable Analysis** \n",
    "3. **Feature Engineering & Preprocessing**\n",
    "4. **Model Training & Selection**\n",
    "5. **Model Evaluation & Metrics**\n",
    "6. **Feature Importance Analysis**\n",
    "7. **Predictions & Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0681a8",
   "metadata": {},
   "source": [
    "## 1. üìÅ Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset - REPLACE 'D:\\data-dojo-1\\datasets\\ecommerce\\customers_messy.csv' with your actual file path\n",
    "df = pd.read_csv('D:\\data-dojo-1\\datasets\\ecommerce\\customers_messy.csv')\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== BASIC INFO ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(missing_data[missing_data > 0])\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "    \n",
    "print(\"\\n=== DUPLICATE ROWS ===\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a44b6",
   "metadata": {},
   "source": [
    "## 2. üéØ Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Define your target variable here\n",
    "# REPLACE 'target_column' with your actual target column name\n",
    "target_column = 'target_column'  # ‚ö†Ô∏è UPDATE THIS WITH YOUR TARGET COLUMN\n",
    "\n",
    "# Check if target column exists\n",
    "if target_column in df.columns:\n",
    "    print(f\"‚úÖ Target variable found: {target_column}\")\n",
    "    \n",
    "    # Analyze target distribution\n",
    "    print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "    target_counts = df[target_column].value_counts()\n",
    "    print(target_counts)\n",
    "    \n",
    "    # Calculate class balance\n",
    "    class_percentages = df[target_column].value_counts(normalize=True) * 100\n",
    "    print(\"\\n=== CLASS PERCENTAGES ===\")\n",
    "    for class_name, percentage in class_percentages.items():\n",
    "        print(f\"{class_name}: {percentage:.2f}%\")\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    target_counts.plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Distribution of {target_column}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(f'Proportion of {target_column}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    min_class_pct = class_percentages.min()\n",
    "    if min_class_pct < 10:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Class imbalance detected! Smallest class: {min_class_pct:.1f}%\")\n",
    "        print(\"Consider using techniques like SMOTE, class weights, or stratified sampling.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Classes are reasonably balanced.\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Column '{target_column}' not found!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    print(\"\\nPlease update the 'target_column' variable above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3490e31",
   "metadata": {},
   "source": [
    "## 3. üìä Feature Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d47e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target (only if target column exists)\n",
    "if target_column in df.columns:\n",
    "    # Identify feature columns (exclude target and ID columns)\n",
    "    id_columns = ['id', 'ID', 'index', 'customer_id', 'user_id']  # Add more ID columns if needed\n",
    "    feature_columns = [col for col in df.columns \n",
    "                      if col != target_column and col not in id_columns]\n",
    "    \n",
    "    X = df[feature_columns].copy()\n",
    "    y = df[target_column].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Features selected: {len(feature_columns)}\")\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Target variable: {target_column}\")\n",
    "    \n",
    "    # Analyze feature types\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüìä Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "    print(f\"üìã Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "    \n",
    "    # Check for missing values in features\n",
    "    feature_missing = X.isnull().sum()\n",
    "    if feature_missing.sum() > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  Missing values in features:\")\n",
    "        print(feature_missing[feature_missing > 0])\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values in features!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Please define the target column first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f7fbf",
   "metadata": {},
   "source": [
    "## 4. üîß Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing pipeline\n",
    "if target_column in df.columns and 'X' in locals():\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"=== HANDLING MISSING VALUES ===\")\n",
    "    \n",
    "    # For numeric features: fill with median\n",
    "    if len(numeric_features) > 0:\n",
    "        numeric_imputer = SimpleImputer(strategy='median')\n",
    "        X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])\n",
    "        print(f\"‚úÖ Filled missing values in numeric features with median\")\n",
    "    \n",
    "    # For categorical features: fill with mode\n",
    "    if len(categorical_features) > 0:\n",
    "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
    "        print(f\"‚úÖ Filled missing values in categorical features with mode\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\n=== ENCODING CATEGORICAL VARIABLES ===\")\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"‚úÖ Encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Encode target variable if it's categorical\n",
    "    target_encoder = None\n",
    "    if y.dtype == 'object':\n",
    "        target_encoder = LabelEncoder()\n",
    "        y = target_encoder.fit_transform(y)\n",
    "        print(f\"\\n‚úÖ Encoded target variable: {target_encoder.classes_}\")\n",
    "    \n",
    "    # Feature scaling (for algorithms that need it)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing completed!\")\n",
    "    print(f\"Final feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target variable shape: {y.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please complete previous steps first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0597f",
   "metadata": {},
   "source": [
    "## 5. üöÇ Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5974ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "if 'X' in locals() and 'y' in locals():\n",
    "    \n",
    "    # Split with stratification to maintain class balance\n",
    "    test_size = 0.2  # 80% train, 20% test\n",
    "    random_state = 42\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    # Also create scaled versions\n",
    "    X_train_scaled, X_test_scaled, _, _ = train_test_split(\n",
    "        X_scaled_df, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(\"=== TRAIN-TEST SPLIT COMPLETED ===\")\n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Check class distribution in splits\n",
    "    print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "    print(\"Training set:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    print(\"\\nTest set:\")\n",
    "    print(pd.Series(y_test).value_counts(normalize=True).sort_index())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please complete preprocessing first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67933b",
   "metadata": {},
   "source": [
    "## 6. ü§ñ Model Training & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8271c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple classification models\n",
    "if 'X_train' in locals():\n",
    "    \n",
    "    # Define models to compare\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Support Vector Machine': SVC(random_state=42, probability=True)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    model_results = {}\n",
    "    \n",
    "    print(\"=== TRAINING MULTIPLE MODELS ===\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Use scaled data for algorithms that need it\n",
    "        if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine']:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if len(np.unique(y)) == 2 else None\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y)) == 2 else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Display results summary\n",
    "    print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(model_results.keys()),\n",
    "        'Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "        'Precision': [results['precision'] for results in model_results.values()],\n",
    "        'Recall': [results['recall'] for results in model_results.values()],\n",
    "        'F1-Score': [results['f1_score'] for results in model_results.values()]\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please complete train-test split first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fec5d9",
   "metadata": {},
   "source": [
    "## 7. üìà Model Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model\n",
    "if 'best_model' in locals():\n",
    "    \n",
    "    print(f\"=== DETAILED EVALUATION: {best_model_name} ===\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nüìä CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, best_predictions))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    plt.subplot(1, 3, 1)\n",
    "    cm = confusion_matrix(y_test, best_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix\\n{best_model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    \n",
    "    # Feature importance (for tree-based models)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        plt.subplot(1, 3, 2)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        top_features = importance_df.head(10)\n",
    "        plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.title('Top 10 Feature Importance')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    # ROC Curve (for binary classification)\n",
    "    if len(np.unique(y)) == 2 and model_results[best_model_name]['probabilities'] is not None:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        fpr, tpr, _ = roc_curve(y_test, model_results[best_model_name]['probabilities'])\n",
    "        auc_score = roc_auc_score(y_test, model_results[best_model_name]['probabilities'])\n",
    "        \n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model performance metrics\n",
    "    accuracy = accuracy_score(y_test, best_predictions)\n",
    "    precision = precision_score(y_test, best_predictions, average='weighted')\n",
    "    recall = recall_score(y_test, best_predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, best_predictions, average='weighted')\n",
    "    \n",
    "    print(\"\\nüéØ FINAL PERFORMANCE METRICS:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    if len(np.unique(y)) == 2 and model_results[best_model_name]['probabilities'] is not None:\n",
    "        auc = roc_auc_score(y_test, model_results[best_model_name]['probabilities'])\n",
    "        print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please complete model training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b08212",
   "metadata": {},
   "source": [
    "## 8. üîç Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4575e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance and model insights\n",
    "if 'best_model' in locals():\n",
    "    \n",
    "    print(f\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        # Create detailed feature importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': best_model.feature_importances_,\n",
    "            'Importance_Percentage': best_model.feature_importances_ * 100\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nüìä TOP 15 MOST IMPORTANT FEATURES:\")\n",
    "        display(importance_df.head(15))\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_20_features = importance_df.head(20)\n",
    "        \n",
    "        plt.barh(range(len(top_20_features)), top_20_features['Importance'])\n",
    "        plt.yticks(range(len(top_20_features)), top_20_features['Feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'Top 20 Feature Importance - {best_model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, v in enumerate(top_20_features['Importance']):\n",
    "            plt.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance insights\n",
    "        print(\"\\nüí° KEY INSIGHTS:\")\n",
    "        top_3_features = importance_df.head(3)\n",
    "        total_importance = top_3_features['Importance'].sum()\n",
    "        print(f\"‚Ä¢ Top 3 features account for {total_importance:.1%} of model decisions\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_3_features.iterrows(), 1):\n",
    "            print(f\"‚Ä¢ #{i} Most important: '{row['Feature']}' ({row['Importance_Percentage']:.1f}%)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Feature importance not available for {best_model_name}\")\n",
    "        \n",
    "        # For linear models, show coefficients\n",
    "        if hasattr(best_model, 'coef_'):\n",
    "            coef_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Coefficient': best_model.coef_[0] if best_model.coef_.ndim > 1 else best_model.coef_\n",
    "            })\n",
    "            coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "            coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "            \n",
    "            print(\"\\nüìä TOP 10 FEATURES BY COEFFICIENT MAGNITUDE:\")\n",
    "            display(coef_df.head(10))\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå Please complete model training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb28502",
   "metadata": {},
   "source": [
    "## 9. üéØ Predictions & Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d066beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and business insights\n",
    "if 'best_model' in locals():\n",
    "    \n",
    "    print(f\"=== PREDICTION ANALYSIS ===\")\n",
    "    \n",
    "    # Create a results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': best_predictions,\n",
    "        'Correct': y_test == best_predictions\n",
    "    })\n",
    "    \n",
    "    # Add original feature values for analysis\n",
    "    test_indices = X_test.index\n",
    "    results_df = results_df.merge(\n",
    "        X.loc[test_indices], \n",
    "        left_index=True, \n",
    "        right_index=True, \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Prediction accuracy by class\n",
    "    print(\"\\nüìä PREDICTION ACCURACY BY CLASS:\")\n",
    "    if target_encoder:\n",
    "        class_accuracy = results_df.groupby('Actual')['Correct'].mean()\n",
    "        for class_idx, accuracy in class_accuracy.items():\n",
    "            class_name = target_encoder.classes_[class_idx]\n",
    "            print(f\"Class '{class_name}': {accuracy:.2%} correct predictions\")\n",
    "    else:\n",
    "        class_accuracy = results_df.groupby('Actual')['Correct'].mean()\n",
    "        for class_val, accuracy in class_accuracy.items():\n",
    "            print(f\"Class {class_val}: {accuracy:.2%} correct predictions\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(\"\\nüîç SAMPLE PREDICTIONS:\")\n",
    "    sample_results = results_df.head(10)[['Actual', 'Predicted', 'Correct']]\n",
    "    display(sample_results)\n",
    "    \n",
    "    # Misclassification analysis\n",
    "    misclassified = results_df[results_df['Correct'] == False]\n",
    "    if len(misclassified) > 0:\n",
    "        print(f\"\\n‚ùå MISCLASSIFIED EXAMPLES: {len(misclassified)} out of {len(results_df)}\")\n",
    "        \n",
    "        # Show patterns in misclassifications\n",
    "        print(\"\\nMost common misclassification patterns:\")\n",
    "        error_patterns = misclassified.groupby(['Actual', 'Predicted']).size().sort_values(ascending=False)\n",
    "        print(error_patterns.head())\n",
    "    \n",
    "    print(\"\\nüéØ MODEL DEPLOYMENT READINESS:\")\n",
    "    print(f\"‚úÖ Model Type: {best_model_name}\")\n",
    "    print(f\"‚úÖ Overall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"‚úÖ Test Set Size: {len(y_test)} samples\")\n",
    "    print(f\"‚úÖ Features Used: {len(X.columns)}\")\n",
    "    \n",
    "    if accuracy >= 0.8:\n",
    "        print(\"üü¢ HIGH ACCURACY: Model ready for deployment!\")\n",
    "    elif accuracy >= 0.7:\n",
    "        print(\"üü° MODERATE ACCURACY: Consider feature engineering or more data\")\n",
    "    else:\n",
    "        print(\"üî¥ LOW ACCURACY: Model needs significant improvement\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please complete model training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d06cf",
   "metadata": {},
   "source": [
    "## 10. ‚úÖ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendations\n",
    "print(\"=== üéØ CLASSIFICATION ANALYSIS COMPLETE ===\")\n",
    "print()\n",
    "\n",
    "if 'best_model' in locals():\n",
    "    print(\"üìä ANALYSIS SUMMARY:\")\n",
    "    print(f\"‚Ä¢ Dataset: {dataset_name}\")\n",
    "    print(f\"‚Ä¢ Target Variable: {target_column}\")\n",
    "    print(f\"‚Ä¢ Best Model: {best_model_name}\")\n",
    "    print(f\"‚Ä¢ Final Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"‚Ä¢ Features Used: {len(X.columns)}\")\n",
    "    print(f\"‚Ä¢ Training Samples: {len(X_train)}\")\n",
    "    print(f\"‚Ä¢ Test Samples: {len(X_test)}\")\n",
    "    \n",
    "    print(\"\\nüöÄ RECOMMENDED NEXT STEPS:\")\n",
    "    \n",
    "    if accuracy >= 0.9:\n",
    "        print(\"1. ‚úÖ Excellent performance! Ready for production deployment\")\n",
    "        print(\"2. üîÑ Set up model monitoring and retraining pipeline\")\n",
    "        print(\"3. üìà Consider A/B testing in production environment\")\n",
    "    elif accuracy >= 0.8:\n",
    "        print(\"1. üéØ Good performance! Consider hyperparameter tuning\")\n",
    "        print(\"2. üîß Try ensemble methods or advanced algorithms\")\n",
    "        print(\"3. üìä Collect more data if possible\")\n",
    "    elif accuracy >= 0.7:\n",
    "        print(\"1. üîß Feature engineering needed - create new features\")\n",
    "        print(\"2. üìä Collect more training data\")\n",
    "        print(\"3. üéØ Try different algorithms or ensemble methods\")\n",
    "        print(\"4. üîç Analyze and fix data quality issues\")\n",
    "    else:\n",
    "        print(\"1. üîç Review data quality and target variable definition\")\n",
    "        print(\"2. üéØ Significant feature engineering required\")\n",
    "        print(\"3. üìä Consider if this is the right ML approach\")\n",
    "        print(\"4. ü§ù Consult domain experts for insights\")\n",
    "    \n",
    "    print(\"\\nüõ†Ô∏è  TECHNICAL IMPROVEMENTS:\")\n",
    "    print(\"‚Ä¢ Hyperparameter tuning with GridSearchCV or RandomSearchCV\")\n",
    "    print(\"‚Ä¢ Cross-validation for more robust evaluation\")\n",
    "    print(\"‚Ä¢ Feature selection techniques (RFE, SelectKBest)\")\n",
    "    print(\"‚Ä¢ Handle class imbalance (SMOTE, class weights)\")\n",
    "    print(\"‚Ä¢ Ensemble methods (Voting, Stacking)\")\n",
    "    print(\"‚Ä¢ Deep learning approaches if dataset is large\")\n",
    "    \n",
    "    print(\"\\nüíæ SAVE YOUR MODEL:\")\n",
    "    print(\"# Uncomment to save the trained model\")\n",
    "    print(\"# import joblib\")\n",
    "    print(\"# joblib.dump(best_model, 'classification_model.pkl')\")\n",
    "    print(\"# print('Model saved successfully!')\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Analysis incomplete. Please run all previous cells.\")\n",
    "\n",
    "print(\"\\nüéâ Classification analysis workflow completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
