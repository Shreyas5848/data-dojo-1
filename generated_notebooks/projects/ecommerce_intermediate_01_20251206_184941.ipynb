{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4d1b62",
   "metadata": {},
   "source": [
    "# ğŸ“ DataDojo Learning Project: Sales Data Feature Engineering\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #1E293B, #0F172A); padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
    "<table style=\"width: 100%; border: none;\">\n",
    "<tr>\n",
    "<td style=\"border: none;\"><strong>ğŸ“ Domain:</strong> Ecommerce</td>\n",
    "<td style=\"border: none;\"><strong>ğŸ“Š Difficulty:</strong> Intermediate</td>\n",
    "<td style=\"border: none;\"><strong>ğŸ“… Generated:</strong> 2025-12-06</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "## ğŸ“‹ Project Overview\n",
    "\n",
    "Prepare sales transaction data for analysis. Create features for customer lifetime value, purchase patterns, and seasonal trends.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By completing this project, you will learn to:\n",
    "\n",
    "- âœ… **Objective 1:** Calculate customer lifetime value (CLV)\n",
    "- âœ… **Objective 2:** Create RFM (Recency, Frequency, Monetary) features\n",
    "- âœ… **Objective 3:** Extract seasonal and temporal features\n",
    "- âœ… **Objective 4:** Engineer product affinity features\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– How to Use This Notebook\n",
    "\n",
    "1. **Read each section carefully** - Educational explanations are provided before each task\n",
    "2. **Run the code cells** - Execute cells in order to see results\n",
    "3. **Complete the exercises** - Fill in the `# YOUR CODE HERE` sections\n",
    "4. **Check your progress** - Use the checkpoint cells to validate your work\n",
    "5. **Track your learning** - Each completed section earns XP!\n",
    "\n",
    "> ğŸ’¡ **Tip:** Don't just run the code - understand WHY each step is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cc2d0",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Step 0: Environment Setup\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d173bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d603b65",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ Step 1: Load and Explore the Dataset\n",
    "\n",
    "### ğŸ“š Learning Concept: Data Loading\n",
    "\n",
    "Before any analysis, we need to:\n",
    "1. **Load the data** from a file (CSV, Excel, JSON, etc.)\n",
    "2. **Inspect the structure** - rows, columns, data types\n",
    "3. **Get a feel for the data** - look at sample rows\n",
    "\n",
    "> ğŸ¯ **Your Task:** Load the dataset and examine its basic properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Dataset path: datasets/ecommerce/transactions.csv\n",
    "\n",
    "df = pd.read_csv('datasets/ecommerce/transactions.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“ Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print()\n",
    "print(\"ğŸ“‹ Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "print(\"\\nğŸ” First 5 rows of the dataset:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"\\nğŸ“Š Data Types and Missing Values:\")\n",
    "print(\"-\" * 60)\n",
    "info_df = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Missing Count': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(info_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bafd8",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ… Checkpoint 1: Data Loading Complete\n",
    "\n",
    "**Questions to verify your understanding:**\n",
    "\n",
    "1. How many rows and columns does the dataset have?\n",
    "2. What are the data types present?\n",
    "3. Are there any missing values?\n",
    "\n",
    "> ğŸ† **Progress:** +15 XP for completing Step 1!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede25b9",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Step 2: Assess Data Quality\n",
    "\n",
    "### ğŸ“š Learning Concept: Data Quality Assessment\n",
    "\n",
    "Data quality issues are common and must be identified before analysis:\n",
    "\n",
    "| Issue | Description | Impact |\n",
    "|-------|-------------|--------|\n",
    "| **Missing Values** | Empty or null entries | Skewed analysis, errors |\n",
    "| **Duplicates** | Repeated records | Inflated counts, bias |\n",
    "| **Outliers** | Extreme values | Distorted statistics |\n",
    "| **Inconsistencies** | Format/spelling variations | Grouping errors |\n",
    "\n",
    "> ğŸ¯ **Your Task:** Identify data quality issues in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality check\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "print(\"\\nğŸ“‰ Missing Values:\")\n",
    "for col in df.columns:\n",
    "    if missing[col] > 0:\n",
    "        print(f\"   â€¢ {col}: {missing[col]:,} ({missing_pct[col]}%)\")\n",
    "if missing.sum() == 0:\n",
    "    print(\"   âœ… No missing values found!\")\n",
    "\n",
    "# Duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nğŸ”„ Duplicate Rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Numeric column statistics\n",
    "print(\"\\nğŸ“Š Numeric Column Statistics:\")\n",
    "print(df.describe().T[['mean', 'std', 'min', 'max']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de22e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Missing values heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "missing_matrix = df.isnull().astype(int)\n",
    "if missing_matrix.sum().sum() > 0:\n",
    "    sns.heatmap(missing_matrix, cbar=True, yticklabels=False, cmap='Reds')\n",
    "    plt.title('Missing Values Heatmap', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No Missing Values!', ha='center', va='center', fontsize=14)\n",
    "    plt.title('Missing Values Check', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Data types distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "type_counts = df.dtypes.astype(str).value_counts()\n",
    "plt.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Data Types Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49af9d",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ… Checkpoint 2: Data Quality Assessment Complete\n",
    "\n",
    "**What did you discover?**\n",
    "\n",
    "Run the cell below to record your findings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207364fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your findings\n",
    "# Fill in the values based on your analysis\n",
    "\n",
    "findings = {\n",
    "    \"total_rows\": len(df),\n",
    "    \"total_columns\": len(df.columns),\n",
    "    \"missing_values_total\": df.isnull().sum().sum(),\n",
    "    \"duplicate_rows\": df.duplicated().sum(),\n",
    "    \"columns_with_missing\": df.columns[df.isnull().any()].tolist()\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ YOUR FINDINGS:\")\n",
    "for key, value in findings.items():\n",
    "    print(f\"   â€¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ† Progress: +15 XP for completing Step 2!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e15ab6",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§¹ Step 3: Clean the Data\n",
    "\n",
    "### ğŸ“š Learning Concept: Data Cleaning Strategies\n",
    "\n",
    "Different issues require different solutions:\n",
    "\n",
    "| Issue | Strategy | Code Example |\n",
    "|-------|----------|--------------|\n",
    "| Missing (numeric) | Fill with mean/median | `df['col'].fillna(df['col'].median())` |\n",
    "| Missing (categorical) | Fill with mode or 'Unknown' | `df['col'].fillna('Unknown')` |\n",
    "| Duplicates | Remove duplicates | `df.drop_duplicates()` |\n",
    "| Outliers | Cap/remove/transform | `df[df['col'] < upper_limit]` |\n",
    "\n",
    "> ğŸ¯ **Your Task:** Apply appropriate cleaning strategies to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Handle Missing Values\n",
    "\n",
    "# Create a copy to preserve original\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Strategy: Fill numeric columns with median, categorical with mode\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        if df_clean[col].dtype in ['int64', 'float64']:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_val, inplace=True)\n",
    "            print(f\"âœ… Filled '{col}' missing values with median: {median_val:.2f}\")\n",
    "        else:\n",
    "            mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "            df_clean[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"âœ… Filled '{col}' missing values with mode: {mode_val}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Missing values after cleaning: {df_clean.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23260a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Remove Duplicates\n",
    "\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "removed = initial_rows - len(df_clean)\n",
    "\n",
    "print(f\"ğŸ”„ Removed {removed:,} duplicate rows\")\n",
    "print(f\"ğŸ“Š Dataset now has {len(df_clean):,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca496ec2",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ… Checkpoint 3: Data Cleaning Complete\n",
    "\n",
    "Let's verify the cleaning was successful:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ce2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning results\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… DATA CLEANING VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“ Original shape: {df.shape}\")\n",
    "print(f\"ğŸ“ Cleaned shape: {df_clean.shape}\")\n",
    "print(f\"\\nğŸ“‰ Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"ğŸ”„ Duplicates: {df_clean.duplicated().sum()}\")\n",
    "\n",
    "if df_clean.isnull().sum().sum() == 0 and df_clean.duplicated().sum() == 0:\n",
    "    print(\"\\nğŸ‰ Data is clean and ready for analysis!\")\n",
    "    print(\"\\nğŸ† Progress: +20 XP for completing Step 3!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some issues remain - review your cleaning steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ef795",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### ğŸ“š Learning Concept: EDA Fundamentals\n",
    "\n",
    "EDA helps us understand patterns, relationships, and insights in data:\n",
    "\n",
    "1. **Univariate Analysis** - Study one variable at a time\n",
    "2. **Bivariate Analysis** - Study relationships between two variables\n",
    "3. **Multivariate Analysis** - Study complex relationships\n",
    "\n",
    "> ğŸ¯ **Your Task:** Explore the data and discover interesting patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis: Numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, min(3, len(numeric_cols)), figsize=(15, 8))\n",
    "    axes = axes.flatten() if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:6]):\n",
    "        if i < len(axes):\n",
    "            # Histogram\n",
    "            df_clean[col].hist(ax=axes[i], bins=30, edgecolor='white', alpha=0.7)\n",
    "            axes[i].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "            axes[i].set_xlabel('')\n",
    "    \n",
    "    plt.suptitle('Distribution of Numeric Variables', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns found for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis: Categorical columns\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, min(3, len(categorical_cols)), figsize=(15, 5))\n",
    "    if len(categorical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols[:3]):\n",
    "        value_counts = df_clean[col].value_counts().head(10)\n",
    "        value_counts.plot(kind='bar', ax=axes[i], color='steelblue', edgecolor='white')\n",
    "        axes[i].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Distribution of Categorical Variables', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No categorical columns found for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis (for numeric columns)\n",
    "if len(numeric_cols) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df_clean[numeric_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strong correlations\n",
    "    print(\"\\nğŸ”— Strong Correlations (|r| > 0.5):\")\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.5:\n",
    "                print(f\"   â€¢ {correlation_matrix.columns[i]} â†” {correlation_matrix.columns[j]}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"Need at least 2 numeric columns for correlation analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ccaaaf",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ… Checkpoint 4: EDA Complete\n",
    "\n",
    "**Key Insights Exercise:**\n",
    "\n",
    "In the cell below, document at least 3 insights you discovered:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d28af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document your insights\n",
    "# YOUR CODE HERE: Replace the examples with your actual findings\n",
    "\n",
    "insights = [\n",
    "    \"Insight 1: [Describe a pattern you found]\",\n",
    "    \"Insight 2: [Describe a relationship you discovered]\", \n",
    "    \"Insight 3: [Describe an interesting distribution]\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ MY KEY INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"\\n{i}. {insight}\")\n",
    "\n",
    "print(\"\\nğŸ† Progress: +25 XP for completing Step 4!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286d9a1",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ Project Completion\n",
    "\n",
    "Congratulations on completing the **Sales Data Feature Engineering** project!\n",
    "\n",
    "### ğŸ“Š Summary of What You Learned:\n",
    "\n",
    "âœ… Calculate customer lifetime value (CLV)\n",
    "âœ… Create RFM (Recency, Frequency, Monetary) features\n",
    "âœ… Extract seasonal and temporal features\n",
    "âœ… Engineer product affinity features\n",
    "\n",
    "### ğŸ† XP Earned This Project:\n",
    "- Step 1 (Data Loading): +15 XP\n",
    "- Step 2 (Quality Assessment): +15 XP\n",
    "- Step 3 (Data Cleaning): +20 XP\n",
    "- Step 4 (EDA): +25 XP\n",
    "- **Project Completion Bonus: +25 XP**\n",
    "\n",
    "**Total: 100 XP** ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "1. **Save your cleaned dataset** for future use\n",
    "2. **Try advanced analysis** techniques\n",
    "3. **Move to the next project** in your learning path\n",
    "\n",
    "Run the cell below to save your work and mark the project complete!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "output_path = 'cleaned_ecommerce_intermediate_01.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"ğŸ’¾ Cleaned dataset saved to: {output_path}\")\n",
    "\n",
    "# Project completion summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ PROJECT COMPLETE: Sales Data Feature Engineering\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸ“Š Original dataset: {len(df):,} rows\")\n",
    "print(f\"ğŸ“Š Cleaned dataset: {len(df_clean):,} rows\")\n",
    "print(f\"ğŸ“Š Data quality: {(1 - df_clean.isnull().sum().sum() / (len(df_clean) * len(df_clean.columns))) * 100:.1f}%\")\n",
    "print(\"\\nğŸ† Total XP Earned: 100 XP\")\n",
    "print(\"\\nğŸ‘‰ Return to DataDojo to mark this project complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "datadojo": {
   "difficulty": "intermediate",
   "domain": "ecommerce",
   "generated_at": "2025-12-06T18:49:41.577910",
   "project_id": "ecommerce_intermediate_01",
   "project_name": "Sales Data Feature Engineering"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
